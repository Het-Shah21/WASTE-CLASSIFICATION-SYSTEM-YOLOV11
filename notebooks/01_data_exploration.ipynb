{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä Task 1: Dataset Download and Exploration\n",
                "\n",
                "## üéØ Objective\n",
                "Download the Waste Classification dataset from Kaggle and perform comprehensive Exploratory Data Analysis (EDA).\n",
                "\n",
                "---\n",
                "\n",
                "## üìö Theory: Why Data Exploration Matters (ML Rule #2, #17)\n",
                "\n",
                "### Martin Zinkevich's Rules Applied:\n",
                "- **Rule #2**: First, design and implement metrics\n",
                "- **Rule #17**: Start with directly observed features\n",
                "\n",
                "### What is EDA?\n",
                "Exploratory Data Analysis is the process of:\n",
                "1. Understanding data distribution\n",
                "2. Identifying patterns and anomalies\n",
                "3. Checking data quality\n",
                "4. Forming hypotheses for modeling\n",
                "\n",
                "### Mathematical Concepts in EDA:\n",
                "\n",
                "**1. Class Distribution (Probability)**\n",
                "```\n",
                "P(class_i) = count(class_i) / total_samples\n",
                "```\n",
                "\n",
                "**2. Image Statistics**\n",
                "- Mean: Œº = (1/n) √ó Œ£x_i\n",
                "- Std Dev: œÉ = ‚àö[(1/n) √ó Œ£(x_i - Œº)¬≤]\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Libraries and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core Libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import shutil\n",
                "from pathlib import Path\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "import cv2\n",
                "\n",
                "# For progress bars\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"‚úÖ Libraries imported successfully!\")\n",
                "print(f\"NumPy version: {np.__version__}\")\n",
                "print(f\"Pandas version: {pd.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Define Project Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Project root directory\n",
                "PROJECT_ROOT = Path(r\"D:\\het\\SELF\\RP\\YOLO-V11-PRO\")\n",
                "\n",
                "# Data directories\n",
                "DATA_DIR = PROJECT_ROOT / \"data\"\n",
                "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
                "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
                "\n",
                "# Create directories if they don't exist\n",
                "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
                "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"üìÅ Project Structure:\")\n",
                "print(f\"   PROJECT_ROOT: {PROJECT_ROOT}\")\n",
                "print(f\"   RAW_DATA_DIR: {RAW_DATA_DIR}\")\n",
                "print(f\"   PROCESSED_DATA_DIR: {PROCESSED_DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Download Dataset from Kaggle\n",
                "\n",
                "### üìù Pre-requisites:\n",
                "1. Create a Kaggle account at https://www.kaggle.com\n",
                "2. Go to Account ‚Üí API ‚Üí Create New API Token\n",
                "3. This downloads `kaggle.json`\n",
                "4. Place it in `~/.kaggle/` (Linux/Mac) or `C:\\Users\\<username>\\.kaggle\\` (Windows)\n",
                "\n",
                "### Alternative: Manual Download\n",
                "If the API doesn't work, download manually from:\n",
                "https://www.kaggle.com/datasets/techsash/waste-classification-data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if Kaggle API is configured\n",
                "import subprocess\n",
                "\n",
                "def check_kaggle_setup():\n",
                "    \"\"\"Check if Kaggle API is properly setup\"\"\"\n",
                "    kaggle_dir = Path.home() / \".kaggle\"\n",
                "    kaggle_json = kaggle_dir / \"kaggle.json\"\n",
                "    \n",
                "    if kaggle_json.exists():\n",
                "        print(\"‚úÖ Kaggle API token found!\")\n",
                "        return True\n",
                "    else:\n",
                "        print(\"‚ùå Kaggle API token not found!\")\n",
                "        print(f\"   Please place kaggle.json in: {kaggle_dir}\")\n",
                "        print(\"\\nüì• Alternative: Download manually from:\")\n",
                "        print(\"   https://www.kaggle.com/datasets/techsash/waste-classification-data\")\n",
                "        print(f\"   Extract to: {RAW_DATA_DIR}\")\n",
                "        return False\n",
                "\n",
                "kaggle_ready = check_kaggle_setup()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download dataset using Kaggle API (run if API is configured)\n",
                "if kaggle_ready:\n",
                "    try:\n",
                "        import kaggle\n",
                "        \n",
                "        print(\"üì• Downloading dataset from Kaggle...\")\n",
                "        kaggle.api.dataset_download_files(\n",
                "            'techsash/waste-classification-data',\n",
                "            path=str(RAW_DATA_DIR),\n",
                "            unzip=True\n",
                "        )\n",
                "        print(\"‚úÖ Dataset downloaded and extracted successfully!\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error downloading: {e}\")\n",
                "        print(\"\\nüì• Please download manually from:\")\n",
                "        print(\"   https://www.kaggle.com/datasets/techsash/waste-classification-data\")\n",
                "else:\n",
                "    print(\"‚è≥ Skipping automatic download. Please download manually.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Explore Dataset Structure\n",
                "\n",
                "### üìê Mathematical Concept: File System as Tree Structure\n",
                "```\n",
                "Dataset Structure:\n",
                "‚îú‚îÄ‚îÄ TRAIN/\n",
                "‚îÇ   ‚îú‚îÄ‚îÄ O/ (Organic)     ‚Üí Class 0\n",
                "‚îÇ   ‚îî‚îÄ‚îÄ R/ (Recyclable)  ‚Üí Class 1\n",
                "‚îî‚îÄ‚îÄ TEST/\n",
                "    ‚îú‚îÄ‚îÄ O/\n",
                "    ‚îî‚îÄ‚îÄ R/\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def explore_directory_structure(path, indent=0):\n",
                "    \"\"\"Recursively explore directory structure\"\"\"\n",
                "    path = Path(path)\n",
                "    if not path.exists():\n",
                "        print(f\"‚ùå Path does not exist: {path}\")\n",
                "        return\n",
                "    \n",
                "    for item in sorted(path.iterdir()):\n",
                "        prefix = \"‚îÇ   \" * indent + \"‚îú‚îÄ‚îÄ \"\n",
                "        if item.is_dir():\n",
                "            # Count files in directory\n",
                "            file_count = len(list(item.rglob(\"*.*\")))\n",
                "            print(f\"{prefix}üìÅ {item.name}/ ({file_count} files)\")\n",
                "            if indent < 2:  # Limit depth\n",
                "                explore_directory_structure(item, indent + 1)\n",
                "        else:\n",
                "            print(f\"{prefix}üìÑ {item.name}\")\n",
                "\n",
                "print(\"\\nüìÇ Dataset Directory Structure:\")\n",
                "print(\"=\" * 50)\n",
                "explore_directory_structure(RAW_DATA_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define dataset paths (adjust based on actual structure)\n",
                "# The dataset might be in a subdirectory after extraction\n",
                "\n",
                "# Try to find the DATASET folder\n",
                "possible_paths = [\n",
                "    RAW_DATA_DIR / \"DATASET\",\n",
                "    RAW_DATA_DIR / \"dataset\",\n",
                "    RAW_DATA_DIR,\n",
                "]\n",
                "\n",
                "DATASET_DIR = None\n",
                "for p in possible_paths:\n",
                "    if (p / \"TRAIN\").exists() or (p / \"train\").exists():\n",
                "        DATASET_DIR = p\n",
                "        break\n",
                "\n",
                "if DATASET_DIR:\n",
                "    print(f\"‚úÖ Dataset found at: {DATASET_DIR}\")\n",
                "    \n",
                "    # Define train and test directories\n",
                "    TRAIN_DIR = DATASET_DIR / \"TRAIN\" if (DATASET_DIR / \"TRAIN\").exists() else DATASET_DIR / \"train\"\n",
                "    TEST_DIR = DATASET_DIR / \"TEST\" if (DATASET_DIR / \"TEST\").exists() else DATASET_DIR / \"test\"\n",
                "    \n",
                "    print(f\"   TRAIN_DIR: {TRAIN_DIR}\")\n",
                "    print(f\"   TEST_DIR: {TEST_DIR}\")\n",
                "else:\n",
                "    print(\"‚ùå Dataset not found! Please check the extraction.\")\n",
                "    print(f\"   Expected location: {RAW_DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Count Images and Analyze Class Distribution\n",
                "\n",
                "### üìê Mathematical Foundation: Class Balance\n",
                "\n",
                "**Class Imbalance Ratio:**\n",
                "```\n",
                "Imbalance Ratio = max(class_count) / min(class_count)\n",
                "```\n",
                "\n",
                "**Why it matters:**\n",
                "- Ratio ‚âà 1: Balanced dataset ‚úÖ\n",
                "- Ratio > 2: Moderately imbalanced ‚ö†Ô∏è\n",
                "- Ratio > 10: Severely imbalanced ‚ùå\n",
                "\n",
                "**Solutions for imbalance:**\n",
                "1. Oversampling minority class\n",
                "2. Undersampling majority class\n",
                "3. Class weights during training\n",
                "4. Data augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_images_in_directory(directory):\n",
                "    \"\"\"\n",
                "    Count images in a directory by class.\n",
                "    \n",
                "    Mathematical representation:\n",
                "    count(class_i) = |{f ‚àà files : f.parent = class_i}|\n",
                "    \"\"\"\n",
                "    directory = Path(directory)\n",
                "    class_counts = {}\n",
                "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}\n",
                "    \n",
                "    if not directory.exists():\n",
                "        print(f\"‚ö†Ô∏è Directory not found: {directory}\")\n",
                "        return class_counts\n",
                "    \n",
                "    for class_folder in directory.iterdir():\n",
                "        if class_folder.is_dir():\n",
                "            count = sum(1 for f in class_folder.iterdir() \n",
                "                       if f.suffix.lower() in image_extensions)\n",
                "            class_counts[class_folder.name] = count\n",
                "    \n",
                "    return class_counts\n",
                "\n",
                "# Count images\n",
                "if DATASET_DIR:\n",
                "    train_counts = count_images_in_directory(TRAIN_DIR)\n",
                "    test_counts = count_images_in_directory(TEST_DIR)\n",
                "    \n",
                "    print(\"\\nüìä Dataset Statistics:\")\n",
                "    print(\"=\" * 50)\n",
                "    print(\"\\nüèãÔ∏è Training Set:\")\n",
                "    for class_name, count in train_counts.items():\n",
                "        class_label = \"Organic\" if class_name.upper() == \"O\" else \"Recyclable\"\n",
                "        print(f\"   {class_name} ({class_label}): {count:,} images\")\n",
                "    print(f\"   Total: {sum(train_counts.values()):,} images\")\n",
                "    \n",
                "    print(\"\\nüß™ Test Set:\")\n",
                "    for class_name, count in test_counts.items():\n",
                "        class_label = \"Organic\" if class_name.upper() == \"O\" else \"Recyclable\"\n",
                "        print(f\"   {class_name} ({class_label}): {count:,} images\")\n",
                "    print(f\"   Total: {sum(test_counts.values()):,} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate class balance metrics using NumPy (from scratch!)\n",
                "def calculate_class_metrics(class_counts):\n",
                "    \"\"\"\n",
                "    Calculate class distribution metrics using NumPy.\n",
                "    \n",
                "    Mathematical formulas:\n",
                "    - Probability: P(class_i) = n_i / N\n",
                "    - Entropy: H = -Œ£ P(i) * log2(P(i))\n",
                "    - Imbalance Ratio: max(counts) / min(counts)\n",
                "    \"\"\"\n",
                "    counts = np.array(list(class_counts.values()))\n",
                "    total = np.sum(counts)\n",
                "    \n",
                "    # Calculate probabilities\n",
                "    probabilities = counts / total\n",
                "    \n",
                "    # Calculate entropy (measure of balance)\n",
                "    # H = -Œ£ P(i) * log2(P(i))\n",
                "    # For 2 classes, max entropy = 1.0 (perfectly balanced)\n",
                "    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
                "    max_entropy = np.log2(len(counts))  # Maximum possible entropy\n",
                "    normalized_entropy = entropy / max_entropy  # 1.0 = perfectly balanced\n",
                "    \n",
                "    # Imbalance ratio\n",
                "    imbalance_ratio = np.max(counts) / np.min(counts)\n",
                "    \n",
                "    return {\n",
                "        'probabilities': probabilities,\n",
                "        'entropy': entropy,\n",
                "        'normalized_entropy': normalized_entropy,\n",
                "        'imbalance_ratio': imbalance_ratio\n",
                "    }\n",
                "\n",
                "if DATASET_DIR and train_counts:\n",
                "    metrics = calculate_class_metrics(train_counts)\n",
                "    \n",
                "    print(\"\\nüìà Class Balance Metrics (Training Set):\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    for i, (class_name, prob) in enumerate(zip(train_counts.keys(), metrics['probabilities'])):\n",
                "        class_label = \"Organic\" if class_name.upper() == \"O\" else \"Recyclable\"\n",
                "        print(f\"   P({class_label}) = {prob:.4f} ({prob*100:.2f}%)\")\n",
                "    \n",
                "    print(f\"\\n   Shannon Entropy: {metrics['entropy']:.4f}\")\n",
                "    print(f\"   Normalized Entropy: {metrics['normalized_entropy']:.4f} (1.0 = perfectly balanced)\")\n",
                "    print(f\"   Imbalance Ratio: {metrics['imbalance_ratio']:.2f}\")\n",
                "    \n",
                "    if metrics['imbalance_ratio'] < 1.5:\n",
                "        print(\"\\n   ‚úÖ Dataset is well-balanced!\")\n",
                "    elif metrics['imbalance_ratio'] < 3:\n",
                "        print(\"\\n   ‚ö†Ô∏è Dataset is slightly imbalanced. Consider using class weights.\")\n",
                "    else:\n",
                "        print(\"\\n   ‚ùå Dataset is significantly imbalanced. Use augmentation/sampling techniques.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Visualize Class Distribution\n",
                "\n",
                "### üìä Visualization Theory\n",
                "Visualizations help us:\n",
                "1. Quickly identify patterns\n",
                "2. Communicate findings effectively\n",
                "3. Detect anomalies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_class_distribution(train_counts, test_counts):\n",
                "    \"\"\"Create comprehensive class distribution visualization\"\"\"\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "    fig.suptitle('üóëÔ∏è Waste Classification Dataset Analysis', fontsize=16, fontweight='bold')\n",
                "    \n",
                "    # Color palette\n",
                "    colors = {'O': '#2ecc71', 'R': '#3498db'}  # Green for Organic, Blue for Recyclable\n",
                "    labels = {'O': 'Organic', 'R': 'Recyclable'}\n",
                "    \n",
                "    # 1. Training Set Bar Chart\n",
                "    ax1 = axes[0, 0]\n",
                "    classes = list(train_counts.keys())\n",
                "    counts = list(train_counts.values())\n",
                "    bars = ax1.bar([labels[c] for c in classes], counts, \n",
                "                   color=[colors[c] for c in classes], edgecolor='black', linewidth=1.5)\n",
                "    ax1.set_title('Training Set Distribution', fontsize=12, fontweight='bold')\n",
                "    ax1.set_ylabel('Number of Images')\n",
                "    \n",
                "    # Add value labels on bars\n",
                "    for bar, count in zip(bars, counts):\n",
                "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,\n",
                "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 2. Test Set Bar Chart\n",
                "    ax2 = axes[0, 1]\n",
                "    test_classes = list(test_counts.keys())\n",
                "    test_count_values = list(test_counts.values())\n",
                "    bars2 = ax2.bar([labels[c] for c in test_classes], test_count_values,\n",
                "                    color=[colors[c] for c in test_classes], edgecolor='black', linewidth=1.5)\n",
                "    ax2.set_title('Test Set Distribution', fontsize=12, fontweight='bold')\n",
                "    ax2.set_ylabel('Number of Images')\n",
                "    \n",
                "    for bar, count in zip(bars2, test_count_values):\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
                "                f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
                "    \n",
                "    # 3. Pie Chart for Training Set\n",
                "    ax3 = axes[1, 0]\n",
                "    explode = (0.05, 0.05)\n",
                "    wedges, texts, autotexts = ax3.pie(counts, explode=explode,\n",
                "                                        labels=[labels[c] for c in classes],\n",
                "                                        colors=[colors[c] for c in classes],\n",
                "                                        autopct='%1.1f%%',\n",
                "                                        shadow=True, startangle=90)\n",
                "    ax3.set_title('Training Set Proportion', fontsize=12, fontweight='bold')\n",
                "    \n",
                "    # 4. Combined Train/Test Comparison\n",
                "    ax4 = axes[1, 1]\n",
                "    x = np.arange(len(classes))\n",
                "    width = 0.35\n",
                "    \n",
                "    bars3 = ax4.bar(x - width/2, counts, width, label='Train', \n",
                "                    color='#3498db', edgecolor='black')\n",
                "    bars4 = ax4.bar(x + width/2, test_count_values, width, label='Test',\n",
                "                    color='#e74c3c', edgecolor='black')\n",
                "    \n",
                "    ax4.set_title('Train vs Test Distribution', fontsize=12, fontweight='bold')\n",
                "    ax4.set_ylabel('Number of Images')\n",
                "    ax4.set_xticks(x)\n",
                "    ax4.set_xticklabels([labels[c] for c in classes])\n",
                "    ax4.legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(PROJECT_ROOT / 'docs' / 'assets' / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(\"\\n‚úÖ Plot saved to: docs/assets/class_distribution.png\")\n",
                "\n",
                "if DATASET_DIR and train_counts and test_counts:\n",
                "    plot_class_distribution(train_counts, test_counts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Display Sample Images\n",
                "\n",
                "### üìê Image Representation Theory\n",
                "\n",
                "**Digital Image as Matrix:**\n",
                "```\n",
                "RGB Image: I ‚àà ‚Ñù^(H √ó W √ó 3)\n",
                "- H: Height (rows)\n",
                "- W: Width (columns)  \n",
                "- 3: Color channels (R, G, B)\n",
                "\n",
                "Pixel value range: [0, 255] for 8-bit images\n",
                "```\n",
                "\n",
                "**Color Channels:**\n",
                "- Red channel: I[:, :, 0]\n",
                "- Green channel: I[:, :, 1]\n",
                "- Blue channel: I[:, :, 2]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_sample_images(directory, n_samples=5):\n",
                "    \"\"\"\n",
                "    Get sample images from each class.\n",
                "    \n",
                "    Returns: Dictionary with class names as keys and list of image paths as values\n",
                "    \"\"\"\n",
                "    directory = Path(directory)\n",
                "    samples = {}\n",
                "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}\n",
                "    \n",
                "    for class_folder in directory.iterdir():\n",
                "        if class_folder.is_dir():\n",
                "            images = [f for f in class_folder.iterdir() \n",
                "                     if f.suffix.lower() in image_extensions]\n",
                "            # Random sample\n",
                "            np.random.seed(42)  # For reproducibility\n",
                "            if len(images) >= n_samples:\n",
                "                indices = np.random.choice(len(images), n_samples, replace=False)\n",
                "                samples[class_folder.name] = [images[i] for i in indices]\n",
                "            else:\n",
                "                samples[class_folder.name] = images\n",
                "    \n",
                "    return samples\n",
                "\n",
                "def display_sample_images(samples, title=\"Sample Images\"):\n",
                "    \"\"\"Display sample images from each class in a grid\"\"\"\n",
                "    \n",
                "    n_classes = len(samples)\n",
                "    n_samples = max(len(imgs) for imgs in samples.values())\n",
                "    \n",
                "    fig, axes = plt.subplots(n_classes, n_samples, figsize=(3*n_samples, 3*n_classes))\n",
                "    fig.suptitle(f'üñºÔ∏è {title}', fontsize=16, fontweight='bold')\n",
                "    \n",
                "    labels = {'O': 'Organic ‚ôªÔ∏è', 'R': 'Recyclable üîÑ'}\n",
                "    \n",
                "    for i, (class_name, images) in enumerate(samples.items()):\n",
                "        for j in range(n_samples):\n",
                "            ax = axes[i, j] if n_classes > 1 else axes[j]\n",
                "            \n",
                "            if j < len(images):\n",
                "                img = Image.open(images[j])\n",
                "                ax.imshow(img)\n",
                "                if j == 0:\n",
                "                    ax.set_ylabel(labels.get(class_name, class_name), fontsize=12, fontweight='bold')\n",
                "                ax.set_title(f'{img.size[0]}x{img.size[1]}', fontsize=9)\n",
                "            \n",
                "            ax.axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(PROJECT_ROOT / 'docs' / 'assets' / 'sample_images.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(\"\\n‚úÖ Sample images saved to: docs/assets/sample_images.png\")\n",
                "\n",
                "if DATASET_DIR:\n",
                "    train_samples = get_sample_images(TRAIN_DIR, n_samples=5)\n",
                "    display_sample_images(train_samples, title=\"Training Set Sample Images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Analyze Image Properties\n",
                "\n",
                "### üìê Statistical Analysis of Images\n",
                "\n",
                "**Key Metrics:**\n",
                "1. **Image Dimensions**: Height √ó Width\n",
                "2. **Aspect Ratio**: Width / Height\n",
                "3. **File Size**: In bytes/KB\n",
                "4. **Color Statistics**: Mean, Std per channel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_image_properties(directory, max_samples=500):\n",
                "    \"\"\"\n",
                "    Analyze properties of images in a directory.\n",
                "    \n",
                "    Returns DataFrame with image properties.\n",
                "    \"\"\"\n",
                "    directory = Path(directory)\n",
                "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}\n",
                "    \n",
                "    data = []\n",
                "    \n",
                "    for class_folder in directory.iterdir():\n",
                "        if class_folder.is_dir():\n",
                "            images = [f for f in class_folder.iterdir() \n",
                "                     if f.suffix.lower() in image_extensions]\n",
                "            \n",
                "            # Sample if too many images\n",
                "            if len(images) > max_samples // 2:\n",
                "                np.random.seed(42)\n",
                "                indices = np.random.choice(len(images), max_samples // 2, replace=False)\n",
                "                images = [images[i] for i in indices]\n",
                "            \n",
                "            for img_path in tqdm(images, desc=f\"Analyzing {class_folder.name}\"):\n",
                "                try:\n",
                "                    img = Image.open(img_path)\n",
                "                    img_array = np.array(img)\n",
                "                    \n",
                "                    # Basic properties\n",
                "                    width, height = img.size\n",
                "                    aspect_ratio = width / height\n",
                "                    file_size = img_path.stat().st_size / 1024  # KB\n",
                "                    \n",
                "                    # Color statistics (if RGB)\n",
                "                    if len(img_array.shape) == 3 and img_array.shape[2] >= 3:\n",
                "                        mean_r = np.mean(img_array[:, :, 0])\n",
                "                        mean_g = np.mean(img_array[:, :, 1])\n",
                "                        mean_b = np.mean(img_array[:, :, 2])\n",
                "                        std_r = np.std(img_array[:, :, 0])\n",
                "                        std_g = np.std(img_array[:, :, 1])\n",
                "                        std_b = np.std(img_array[:, :, 2])\n",
                "                    else:\n",
                "                        mean_r = mean_g = mean_b = np.mean(img_array)\n",
                "                        std_r = std_g = std_b = np.std(img_array)\n",
                "                    \n",
                "                    data.append({\n",
                "                        'class': class_folder.name,\n",
                "                        'filename': img_path.name,\n",
                "                        'width': width,\n",
                "                        'height': height,\n",
                "                        'aspect_ratio': aspect_ratio,\n",
                "                        'file_size_kb': file_size,\n",
                "                        'mean_r': mean_r,\n",
                "                        'mean_g': mean_g,\n",
                "                        'mean_b': mean_b,\n",
                "                        'std_r': std_r,\n",
                "                        'std_g': std_g,\n",
                "                        'std_b': std_b,\n",
                "                        'brightness': (mean_r + mean_g + mean_b) / 3\n",
                "                    })\n",
                "                except Exception as e:\n",
                "                    print(f\"Error processing {img_path}: {e}\")\n",
                "    \n",
                "    return pd.DataFrame(data)\n",
                "\n",
                "if DATASET_DIR:\n",
                "    print(\"\\nüìä Analyzing image properties (this may take a minute)...\")\n",
                "    image_df = analyze_image_properties(TRAIN_DIR, max_samples=500)\n",
                "    print(f\"\\n‚úÖ Analyzed {len(image_df)} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display summary statistics\n",
                "if DATASET_DIR and len(image_df) > 0:\n",
                "    print(\"\\nüìä Image Property Statistics:\")\n",
                "    print(\"=\" * 60)\n",
                "    \n",
                "    # Group by class\n",
                "    class_stats = image_df.groupby('class').agg({\n",
                "        'width': ['mean', 'min', 'max', 'std'],\n",
                "        'height': ['mean', 'min', 'max', 'std'],\n",
                "        'aspect_ratio': ['mean', 'std'],\n",
                "        'file_size_kb': ['mean', 'min', 'max'],\n",
                "        'brightness': ['mean', 'std']\n",
                "    }).round(2)\n",
                "    \n",
                "    print(class_stats)\n",
                "    \n",
                "    # Overall statistics\n",
                "    print(\"\\nüìà Overall Statistics:\")\n",
                "    print(f\"   Average Width: {image_df['width'].mean():.0f} px\")\n",
                "    print(f\"   Average Height: {image_df['height'].mean():.0f} px\")\n",
                "    print(f\"   Width Range: [{image_df['width'].min()}, {image_df['width'].max()}] px\")\n",
                "    print(f\"   Height Range: [{image_df['height'].min()}, {image_df['height'].max()}] px\")\n",
                "    print(f\"   Average File Size: {image_df['file_size_kb'].mean():.1f} KB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_image_statistics(df):\n",
                "    \"\"\"Create comprehensive visualization of image properties\"\"\"\n",
                "    \n",
                "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "    fig.suptitle('üìä Image Property Analysis', fontsize=16, fontweight='bold')\n",
                "    \n",
                "    colors = {'O': '#2ecc71', 'R': '#3498db'}\n",
                "    labels = {'O': 'Organic', 'R': 'Recyclable'}\n",
                "    \n",
                "    # 1. Image Width Distribution\n",
                "    ax1 = axes[0, 0]\n",
                "    for class_name in df['class'].unique():\n",
                "        class_df = df[df['class'] == class_name]\n",
                "        ax1.hist(class_df['width'], bins=30, alpha=0.6, \n",
                "                label=labels.get(class_name, class_name), color=colors.get(class_name, 'gray'))\n",
                "    ax1.set_title('Image Width Distribution')\n",
                "    ax1.set_xlabel('Width (pixels)')\n",
                "    ax1.set_ylabel('Frequency')\n",
                "    ax1.legend()\n",
                "    \n",
                "    # 2. Image Height Distribution\n",
                "    ax2 = axes[0, 1]\n",
                "    for class_name in df['class'].unique():\n",
                "        class_df = df[df['class'] == class_name]\n",
                "        ax2.hist(class_df['height'], bins=30, alpha=0.6,\n",
                "                label=labels.get(class_name, class_name), color=colors.get(class_name, 'gray'))\n",
                "    ax2.set_title('Image Height Distribution')\n",
                "    ax2.set_xlabel('Height (pixels)')\n",
                "    ax2.set_ylabel('Frequency')\n",
                "    ax2.legend()\n",
                "    \n",
                "    # 3. Aspect Ratio Distribution\n",
                "    ax3 = axes[0, 2]\n",
                "    for class_name in df['class'].unique():\n",
                "        class_df = df[df['class'] == class_name]\n",
                "        ax3.hist(class_df['aspect_ratio'], bins=30, alpha=0.6,\n",
                "                label=labels.get(class_name, class_name), color=colors.get(class_name, 'gray'))\n",
                "    ax3.axvline(x=1.0, color='red', linestyle='--', label='Square (1:1)')\n",
                "    ax3.set_title('Aspect Ratio Distribution')\n",
                "    ax3.set_xlabel('Aspect Ratio (W/H)')\n",
                "    ax3.set_ylabel('Frequency')\n",
                "    ax3.legend()\n",
                "    \n",
                "    # 4. Width vs Height Scatter\n",
                "    ax4 = axes[1, 0]\n",
                "    for class_name in df['class'].unique():\n",
                "        class_df = df[df['class'] == class_name]\n",
                "        ax4.scatter(class_df['width'], class_df['height'], alpha=0.5, \n",
                "                   label=labels.get(class_name, class_name), color=colors.get(class_name, 'gray'), s=20)\n",
                "    ax4.set_title('Width vs Height')\n",
                "    ax4.set_xlabel('Width (pixels)')\n",
                "    ax4.set_ylabel('Height (pixels)')\n",
                "    ax4.legend()\n",
                "    \n",
                "    # 5. Brightness Distribution by Class\n",
                "    ax5 = axes[1, 1]\n",
                "    brightness_data = [df[df['class'] == c]['brightness'].values for c in df['class'].unique()]\n",
                "    bp = ax5.boxplot(brightness_data, labels=[labels.get(c, c) for c in df['class'].unique()],\n",
                "                     patch_artist=True)\n",
                "    for patch, class_name in zip(bp['boxes'], df['class'].unique()):\n",
                "        patch.set_facecolor(colors.get(class_name, 'gray'))\n",
                "    ax5.set_title('Brightness Distribution by Class')\n",
                "    ax5.set_ylabel('Mean Brightness')\n",
                "    \n",
                "    # 6. File Size Distribution\n",
                "    ax6 = axes[1, 2]\n",
                "    for class_name in df['class'].unique():\n",
                "        class_df = df[df['class'] == class_name]\n",
                "        ax6.hist(class_df['file_size_kb'], bins=30, alpha=0.6,\n",
                "                label=labels.get(class_name, class_name), color=colors.get(class_name, 'gray'))\n",
                "    ax6.set_title('File Size Distribution')\n",
                "    ax6.set_xlabel('File Size (KB)')\n",
                "    ax6.set_ylabel('Frequency')\n",
                "    ax6.legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(PROJECT_ROOT / 'docs' / 'assets' / 'image_statistics.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(\"\\n‚úÖ Statistics plot saved to: docs/assets/image_statistics.png\")\n",
                "\n",
                "if DATASET_DIR and len(image_df) > 0:\n",
                "    plot_image_statistics(image_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Color Channel Analysis\n",
                "\n",
                "### üìê RGB Color Space Mathematics\n",
                "\n",
                "**Color Model:**\n",
                "```\n",
                "RGB Color = (R, G, B) where R, G, B ‚àà [0, 255]\n",
                "\n",
                "Total possible colors = 256¬≥ = 16,777,216\n",
                "```\n",
                "\n",
                "**Channel Statistics:**\n",
                "- Mean intensity per channel reveals color dominance\n",
                "- Standard deviation indicates color variance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_color_analysis(df):\n",
                "    \"\"\"Analyze and visualize color distribution by class\"\"\"\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    fig.suptitle('üé® Color Channel Analysis by Class', fontsize=14, fontweight='bold')\n",
                "    \n",
                "    labels = {'O': 'Organic', 'R': 'Recyclable'}\n",
                "    \n",
                "    # 1. Mean RGB values by class\n",
                "    ax1 = axes[0]\n",
                "    classes = df['class'].unique()\n",
                "    x = np.arange(len(classes))\n",
                "    width = 0.25\n",
                "    \n",
                "    means_r = [df[df['class'] == c]['mean_r'].mean() for c in classes]\n",
                "    means_g = [df[df['class'] == c]['mean_g'].mean() for c in classes]\n",
                "    means_b = [df[df['class'] == c]['mean_b'].mean() for c in classes]\n",
                "    \n",
                "    ax1.bar(x - width, means_r, width, label='Red', color='#e74c3c')\n",
                "    ax1.bar(x, means_g, width, label='Green', color='#2ecc71')\n",
                "    ax1.bar(x + width, means_b, width, label='Blue', color='#3498db')\n",
                "    \n",
                "    ax1.set_title('Mean Color Channel Values by Class')\n",
                "    ax1.set_ylabel('Mean Intensity (0-255)')\n",
                "    ax1.set_xticks(x)\n",
                "    ax1.set_xticklabels([labels.get(c, c) for c in classes])\n",
                "    ax1.legend()\n",
                "    ax1.set_ylim(0, 255)\n",
                "    \n",
                "    # 2. Color space scatter (R vs G with B as hue)\n",
                "    ax2 = axes[1]\n",
                "    scatter = ax2.scatter(df['mean_r'], df['mean_g'], c=df['mean_b'], \n",
                "                         cmap='viridis', alpha=0.6, s=30)\n",
                "    plt.colorbar(scatter, ax=ax2, label='Blue Channel Mean')\n",
                "    ax2.set_title('Color Space Distribution (R vs G, colored by B)')\n",
                "    ax2.set_xlabel('Red Channel Mean')\n",
                "    ax2.set_ylabel('Green Channel Mean')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(PROJECT_ROOT / 'docs' / 'assets' / 'color_analysis.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print(\"\\n‚úÖ Color analysis saved to: docs/assets/color_analysis.png\")\n",
                "\n",
                "if DATASET_DIR and len(image_df) > 0:\n",
                "    plot_color_analysis(image_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Save Analysis Results\n",
                "\n",
                "### üíæ Data Persistence (ML Rule #11: Documentation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save analysis results\n",
                "if DATASET_DIR and len(image_df) > 0:\n",
                "    # Save DataFrame to CSV\n",
                "    analysis_path = PROJECT_ROOT / 'data' / 'image_analysis.csv'\n",
                "    image_df.to_csv(analysis_path, index=False)\n",
                "    print(f\"‚úÖ Image analysis saved to: {analysis_path}\")\n",
                "    \n",
                "    # Create summary report\n",
                "    summary = {\n",
                "        'dataset_name': 'Waste Classification',\n",
                "        'total_train_images': sum(train_counts.values()),\n",
                "        'total_test_images': sum(test_counts.values()),\n",
                "        'classes': list(train_counts.keys()),\n",
                "        'class_counts_train': train_counts,\n",
                "        'class_counts_test': test_counts,\n",
                "        'avg_width': image_df['width'].mean(),\n",
                "        'avg_height': image_df['height'].mean(),\n",
                "        'avg_file_size_kb': image_df['file_size_kb'].mean(),\n",
                "        'analyzed_samples': len(image_df)\n",
                "    }\n",
                "    \n",
                "    # Save as JSON\n",
                "    import json\n",
                "    summary_path = PROJECT_ROOT / 'data' / 'dataset_summary.json'\n",
                "    with open(summary_path, 'w') as f:\n",
                "        json.dump(summary, f, indent=2, default=str)\n",
                "    print(f\"‚úÖ Dataset summary saved to: {summary_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Summary & Key Findings\n",
                "\n",
                "### What We Learned:\n",
                "1. **Dataset Size**: ~22,564 training + ~2,513 test images\n",
                "2. **Classes**: 2 (Organic and Recyclable)\n",
                "3. **Class Balance**: Check the imbalance ratio from above\n",
                "4. **Image Dimensions**: Variable sizes (may need resizing for YOLO)\n",
                "5. **Color Patterns**: Different color profiles for each class\n",
                "\n",
                "### Next Steps:\n",
                "- **Task 2**: Data preprocessing and augmentation\n",
                "- Resize images to consistent dimensions\n",
                "- Apply data augmentation techniques\n",
                "\n",
                "---\n",
                "\n",
                "## üìö Learning Resources\n",
                "\n",
                "### Theory:\n",
                "- [Understanding Data Exploration](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)\n",
                "- [Image Processing Basics](https://homepages.inf.ed.ac.uk/rbf/HIPR2/wksheets.htm)\n",
                "\n",
                "### Videos:\n",
                "- [StatQuest: Histograms](https://www.youtube.com/watch?v=qBigTkBLU6g)\n",
                "- [3Blue1Brown: Linear Algebra (for image matrix concepts)](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
                "\n",
                "### Code Reference:\n",
                "See `docs/CODE-THEORY.md` Section 1.1-1.2 for mathematical foundations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ TASK 1 COMPLETE: Dataset Download and Exploration\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nüìã What was accomplished:\")\n",
                "print(\"   ‚úì Dataset downloaded/verified from Kaggle\")\n",
                "print(\"   ‚úì Directory structure explored\")\n",
                "print(\"   ‚úì Class distribution analyzed\")\n",
                "print(\"   ‚úì Sample images visualized\")\n",
                "print(\"   ‚úì Image properties analyzed (dimensions, colors)\")\n",
                "print(\"   ‚úì Analysis results saved\")\n",
                "print(\"\\nüìÅ Generated files:\")\n",
                "print(\"   - docs/assets/class_distribution.png\")\n",
                "print(\"   - docs/assets/sample_images.png\")\n",
                "print(\"   - docs/assets/image_statistics.png\")\n",
                "print(\"   - docs/assets/color_analysis.png\")\n",
                "print(\"   - data/image_analysis.csv\")\n",
                "print(\"   - data/dataset_summary.json\")\n",
                "print(\"\\n‚û°Ô∏è Ready for Task 2: Data Preprocessing and Augmentation\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}